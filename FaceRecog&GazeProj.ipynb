{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f92de96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "def analyzeFaces():\n",
    "    # Initialize the camera\n",
    "    cap = cv2.VideoCapture(0)  # 0 is typically the default camera\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Camera could not be accessed.\")\n",
    "        return {}\n",
    "\n",
    "    # Capture a single frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "    # Check if the frame was captured successfully\n",
    "    if not ret:\n",
    "        print(\"Error: No frame captured.\")\n",
    "        return {}\n",
    "\n",
    "    face_encodings = face_recognition.face_encodings(frame)\n",
    "\n",
    "    # Dictionary to store face data\n",
    "    face_data = {}\n",
    "\n",
    "    # Generate face data\n",
    "    for i, face_encoding in enumerate(face_encodings):\n",
    "        # Assume each face ID is unique based on its encoding\n",
    "        face_id = f\"face_{i+1}\"\n",
    "\n",
    "        # Analyze the face to determine if it's looking at the camera (simple heuristic: if the face is in the central part of the frame)\n",
    "        top, right, bottom, left = face_locations[i]\n",
    "        face_center_x = (left + right) // 2\n",
    "        face_center_y = (top + bottom) // 2\n",
    "        frame_center_x, frame_center_y = frame.shape[1] // 2, frame.shape[0] // 2\n",
    "\n",
    "        # Check if the center of the face is within 20% of the center of the frame to consider it looking at/near the camera\n",
    "        if abs(face_center_x - frame_center_x) < frame.shape[1] * 0.2 and abs(face_center_y - frame_center_y) < frame.shape[0] * 0.2:\n",
    "            face_data[face_id] = {\n",
    "                \"face_location\": (top, right, bottom, left),\n",
    "                \"face_encoding\": face_encoding.tolist(),  # Convert numpy array to list for serialization\n",
    "                \"looking_at_camera\": True\n",
    "            }\n",
    "        else:\n",
    "            face_data[face_id] = {\n",
    "                \"face_location\": (top, right, bottom, left),\n",
    "                \"face_encoding\": face_encoding.tolist(),\n",
    "                \"looking_at_camera\": False\n",
    "            }\n",
    "\n",
    "    return face_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbcd20f-8528-49fa-80e6-93ff6b3380c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "faces = analyzeFaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba06988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "# Placeholder function for face detection and identification\n",
    "def identify_faces(frame):\n",
    "    # Your face detection and identification logic here\n",
    "    # Return a dictionary of detected face IDs with their positions\n",
    "    # return {\"face_id1\": (x, y, w, h)}\n",
    "    return\n",
    "\n",
    "# Initialize system info retrieval and logging setup here (from previous steps)\n",
    "\n",
    "faces_last_seen = {}  # Track when each face was last seen\n",
    "face_absence_start = {}  # Track when a face is no longer seen\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        identified_faces = identify_faces(frame)\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update or add detected faces with the current timestamp\n",
    "        for face_id in identified_faces:\n",
    "            if face_id not in faces_last_seen:\n",
    "                logging.info(f\"Face {face_id} detected at {datetime.datetime.now()}.\")\n",
    "            faces_last_seen[face_id] = current_time\n",
    "            if face_id in face_absence_start:\n",
    "                del face_absence_start[face_id]  # Face reappeared; remove from absence tracking\n",
    "\n",
    "        # Check for faces that are no longer visible\n",
    "        for face_id in list(faces_last_seen):\n",
    "            if face_id not in identified_faces:\n",
    "                if face_id not in face_absence_start:\n",
    "                    face_absence_start[face_id] = current_time  # Start tracking absence\n",
    "                elif current_time - face_absence_start[face_id] > 900:  # 15 minutes in seconds\n",
    "                    logging.info(f\"Face {face_id} has been out of view for more than 15 minutes.\")\n",
    "                    del faces_last_seen[face_id]  # Remove from tracking\n",
    "                    del face_absence_start[face_id]  # Reset absence tracking\n",
    "        \n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dee151-00bb-4a6a-8855-fe34b7339d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def takePic():\n",
    "    cap = cv2.VideoCapture(0)  # 0 is typically the default camera\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Camera could not be accessed.\")\n",
    "        return {}\n",
    "\n",
    "    # Capture a single frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the camera\n",
    "    cap.release()\n",
    "\n",
    "    # Check if the frame was captured successfully\n",
    "    if not ret:\n",
    "        print(\"Error: No frame captured.\")\n",
    "        return {}\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07e84736-4ee7-4ebb-a08d-ff39cf330736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "frame4 = takePic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ade05f5e-c37a-42f7-807f-d0c4b801527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('frame', frame4)\n",
    "cv2.waitKey(5000) # 5 seconds\n",
    "cv2.destroyAllWindows() # close when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e075ade0-d6b5-411b-9440-9395117f459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(image_height, face_size, avg_size):\n",
    "    size_ratio = image_height/face_size\n",
    "    plane_height = avg_size*size_ratio #total height of plane perpendicular to camera anngle where face is positioned\n",
    "\n",
    "    z_distance = (plane_height/2)*1.327044822  # found using trigonometry\n",
    "    \n",
    "    return z_distance, plane_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2d16fd-d04c-4b33-b013-ae8a01cd4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_face_position(image_width, image_height, face_size_x, face_size_y, face_center_x, face_center_y, image_center_x, image_center_y):\n",
    "    z_distance1, plane_height = calculate_distance(image_height, face_size_y, 0.12)\n",
    "    z_distance2, plane_width = calculate_distance(image_width, face_size_x, 0.125)\n",
    "    z_distance = (z_distance1 + z_distance2)/2\n",
    "\n",
    "    # Change x and y to interpretable lengths(meters)\n",
    "    world_y = (( face_center_y - image_center_y)/image_height) * plane_height\n",
    "    world_x = (( face_center_x - image_center_x)/image_width) * plane_width\n",
    "    face_position = [world_x, world_y, z_distance]  # Position is meters away from camera in each dimension\n",
    "\n",
    "    return face_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b40b94ac-4337-42f0-8899-f4408d66c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isGazeClose(face_pos, vector, threshold):\n",
    "    p = np.array(face_pos)  # Face position\n",
    "    v = np.array(vector)  # Normalized direction vector\n",
    "\n",
    "    # Calculate t for the closest point\n",
    "    t = -np.dot(p, v)\n",
    "\n",
    "    # Calculate the closest point\n",
    "    closest_point = p + t * v\n",
    "    print(\"Closest Point is:\")\n",
    "    print(closest_point)\n",
    "\n",
    "    # Calculate the distance from the camera to this closest point\n",
    "    distance = (closest_point[0]**2+closest_point[1]**2+closest_point[2]**2)**0.5\n",
    "\n",
    "    print(\"Distance is:\")\n",
    "    print(distance)\n",
    "    print(\"Threshold is:\")\n",
    "    print(threshold)\n",
    "\n",
    "    # Determine if this distance is within the acceptable threshold\n",
    "    if distance is not None and threshold is not None:\n",
    "        return distance < threshold\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c4fe1d9-57d6-49d8-965b-6d31d67ffe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_threshold(confidence, z_distance):\n",
    "    threshold = (1-confidence)*z_distance\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67363f0a-894e-4984-b6d0-b870cbd7cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def cameraSpying(image):\n",
    "    if image is None:\n",
    "        return \"Input in CameraSpying is Null\"\n",
    "\n",
    "    # Detect face landmarks\n",
    "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    \n",
    "    if not face_landmarks_list:\n",
    "        return \"No faces detected\"\n",
    "\n",
    "    # Get image dimensions and center\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    img_center_x, img_center_y = img_width // 2, img_height // 2\n",
    "\n",
    "    faces_dict = {}\n",
    "\n",
    "    # Process each face found\n",
    "    for face_landmarks, (top, right, bottom, left) in zip(face_landmarks_list, face_locations):\n",
    "        \n",
    "        face_center_x = (left + right) // 2\n",
    "        face_center_y = (top + bottom) // 2\n",
    "        face_size_y = bottom - top\n",
    "        face_size_x = right - left\n",
    "        \n",
    "        isLooking = False\n",
    "        \n",
    "        # Extract eye regions and calculate pupil centers\n",
    "        for eye_key in ['left_eye', 'right_eye']:\n",
    "            eye_points = face_landmarks[eye_key]\n",
    "            eye_array = np.array(eye_points, dtype=np.int32)\n",
    "\n",
    "            # Calculate the eyeball center by finding the average of the eye corner landmarks\n",
    "            leftmost = min(eye_points, key=lambda x: x[0])\n",
    "            rightmost = max(eye_points, key=lambda x: x[0])\n",
    "            topmost = min(eye_points, key=lambda x: x[1])\n",
    "            bottommost = max(eye_points, key=lambda x: x[1])\n",
    "\n",
    "            eye_center_x = (leftmost[0] + rightmost[0]) // 2\n",
    "            eye_center_y = (topmost[1] + bottommost[1]) // 2\n",
    "            \n",
    "            # Create a mask to isolate the eye area\n",
    "            mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "            cv2.fillConvexPoly(mask, eye_array, 255)\n",
    "            eye_region = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "            # Convert eye region to grayscale and apply GaussianBlur\n",
    "            gray_eye = cv2.cvtColor(eye_region, cv2.COLOR_BGR2GRAY)\n",
    "            blurred_eye = cv2.GaussianBlur(gray_eye, (7, 7), 0)\n",
    "\n",
    "            # Find the darkest point in the eye region, which is the pupil\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(blurred_eye, mask=mask)\n",
    "            pupil_x, pupil_y = min_loc\n",
    "\n",
    "            eyeFaceRatio = (1.15/12) # approximate ratio of eyeball radius to vertical face length\n",
    "            eyeRadius = eyeFaceRatio * face_size_y\n",
    "            \n",
    "            change_in_z = (eyeRadius**2 - (pupil_y - eye_center_y)**2 - (pupil_x - eye_center_x)**2)**0.5\n",
    "\n",
    "            slope_vector = [(pupil_x - eye_center_x)/eyeRadius, (pupil_y - eye_center_y)/eyeRadius, change_in_z/eyeRadius] #normalized to have length 1(pixel)\n",
    "            print(\"Slope Vector:\")\n",
    "            print(slope_vector)\n",
    "                  \n",
    "            \n",
    "            face_pos = determine_face_position(img_width, img_height, face_size_x, face_size_y, face_center_x, face_center_y, img_center_x, img_center_y)\n",
    "            print(\"Face position is:\")\n",
    "            print(face_pos)\n",
    "            \n",
    "            confidence = 0.3 #adjust depending on what confidence you want for gaze\n",
    "            threshold = determine_threshold(confidence, face_pos[2])\n",
    "            \n",
    "            looking = isGazeClose(face_pos, slope_vector, threshold) #boolean\n",
    "\n",
    "            if looking is None:\n",
    "                print(\"Gaze not found\")\n",
    "            \n",
    "            if looking:\n",
    "                isLooking = True\n",
    "\n",
    "        faces_dict[1] = {\"face_location\" : (face_center_x, face_center_y, img_height, img_width), \"Looking\": isLooking}\n",
    "    return faces_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "708c5828-fb5a-4e15-a9e2-c9e9426b8812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope Vector:\n",
      "[-1.0077204388459975, -0.03250711093051605, (7.879056949944455e-18+0.12867476492700036j)]\n",
      "Face position is:\n",
      "[0.017523364485981307, 0.020186915887850466, 0.14221290304299064]\n",
      "Closest Point is:\n",
      "[-0.00093291+0.01844049j  0.01959155+0.00059485j  0.14456755+0.00235666j]\n",
      "Distance is:\n",
      "(0.14472000352949335+0.0023158353656242037j)\n",
      "Threshold is:\n",
      "0.09954903213009345\n",
      "Slope Vector:\n",
      "[1.0727346607070296, 0.0650142218610321, (2.4106146599199328e-17+0.3936832499947408j)]\n",
      "Face position is:\n",
      "[0.017523364485981307, 0.020186915887850466, 0.14221290304299064]\n",
      "Closest Point is:\n",
      "[-0.00404971-0.06005902j  0.01887946-0.00363994j  0.16425398-0.00791711j]\n",
      "Distance is:\n",
      "(0.15402151483080156-0.007310117048639081j)\n",
      "Threshold is:\n",
      "0.09954903213009345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: {'face_location': (365, 294, 480, 640), 'Looking': False}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cameraSpying(takePic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b647f90e-b83f-4e9d-8444-5784ee85b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('frame', frame4)\n",
    "cv2.waitKey(0) # until a key is pressed\n",
    "cv2.destroyAllWindows() # close when done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
